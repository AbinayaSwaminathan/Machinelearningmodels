{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from xgboost) (1.5.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from lightgbm) (0.35.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from lightgbm) (0.23.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from lightgbm) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\dhrumil\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (0.17.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost\n",
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"E:/study/Masters Program/Computer Science/Summers/TOCS/HW3/optdigits.tra\",header=None)\n",
    "test_df = pd.read_csv(\"E:/study/Masters Program/Computer Science/Summers/TOCS/HW3/optdigits.tes\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 65)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X25_EachClass = train_df[train_df[64] == 0].sample(25)\n",
    "for i in range(1,10):\n",
    "        X25_EachClass = X25_EachClass.append(train_df[train_df[64] == i].sample(25))\n",
    "X25_EachClass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 65)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X100_EachClass = train_df[train_df[64] == 0].sample(100)\n",
    "for i in range(1,10):\n",
    "        X100_EachClass = X100_EachClass.append(train_df[train_df[64] == i].sample(100))\n",
    "X100_EachClass.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for X25 instances with max_depth: 2 is: 0.7000556483027267\n",
      "Error for X25 instances with max_depth: 3 is: 0.4485253199777407\n",
      "Error for X25 instances with max_depth: 5 is: 0.30606566499721755\n",
      "Error for X25 instances with max_depth: 10 is: 0.27378964941569284\n",
      "\n",
      "Error for X100 instances with max_depth: 2 is: 0.6533110740122426\n",
      "Error for X100 instances with max_depth: 3 is: 0.4668892598775737\n",
      "Error for X100 instances with max_depth: 5 is: 0.28603227601558157\n",
      "Error for X100 instances with max_depth: 10 is: 0.21814134668892604\n",
      "\n",
      "Error for the entire dataset with max_depth: 2 is: 0.664440734557596\n",
      "Error for the entire dataset with max_depth: 3 is: 0.4652198107957707\n",
      "Error for the entire dataset with max_depth: 5 is: 0.23205342237061766\n",
      "Error for the entire dataset with max_depth: 10 is: 0.11908736783528107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_depth = [2,3,5,10]\n",
    "\n",
    "##------------- X25 Instances for each class------------------\n",
    "\n",
    "xtrain = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for X25 instances with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "    \n",
    "print('')\n",
    "#--------------X100 Instances for each class-----------------------\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for X100 instances with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "print('')    \n",
    "\n",
    "\n",
    "#--------------Entire training dataset-----------------------\n",
    "\n",
    "xtrain = train_df[train_df.columns[0:64]]\n",
    "ytrain = train_df[train_df.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for the entire dataset with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters and their default values for the XGboost model: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=4, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "\n",
      "\n",
      "Test Error for X100 with default parameter setting: 0.07623817473567052\n",
      "Test Error for X25 with default parameter setting: 0.13355592654424042\n",
      "Test Error for X100 with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0: 0.900946021146355\n",
      "Test Error for X25 with with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0: 0.900946021146355\n",
      "Test Error for X100 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting: 0.08124652198107962\n",
      "Test Error for X25 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting: 0.15804117974401777\n",
      "Test Error for X100 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.08069003895381188\n",
      "Test Error for X25 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.14746800222593215\n",
      "Test Error for X100 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.08903728436282698\n",
      "Test Error for X25 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.12465219810795769\n"
     ]
    }
   ],
   "source": [
    "########--------XGBOOST for X100 and X25 with different parameter settings------\n",
    "\n",
    "\n",
    "xtrain25 = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain25 = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest25 = test_df\n",
    "ytest25 = xtest25.copy()\n",
    "xtest25 = xtest25[xtest25.columns[0:64]]\n",
    "ytest25 = ytest25[ytest25.columns[64]]\n",
    "\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "\n",
    "\n",
    "#------  default parameter values training of the dataset--------\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(xtrain,ytrain)\n",
    "print(\"Parameters and their default values for the XGboost model:\",model)\n",
    "print('')\n",
    "print('')\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print(\"Test Error for X100 with default parameter setting:\",error)\n",
    "\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with default parameter setting:\", error25)\n",
    "\n",
    "#---------booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=5,min_child_weight=2,subsample=0,verbosity=0)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0:\", error25)\n",
    "\n",
    "\n",
    "#---------booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting:\", error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting:\", error25)\n",
    "\n",
    "#---------tree_method=exact,max_depth=7,min_child_weight=6,scale_pos_weight=0,refresh_leaf=0\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,tree_method='exact',max_depth=7,min_child_weight=6,scale_pos_weight=0,refresh_leaf=0)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print(\"Test Error for X100 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error25)\n",
    "\n",
    "#---------booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters and their default values for the LightGBM model: LGBMClassifier()\n",
      "\n",
      "\n",
      "1st set of parameters:\n",
      "Test Error for X100 with default parameter setting: 0.07122982749026152\n",
      "Test Error for X25 with default parameter setting: 0.12298274902615469\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "2nd set of parameters:\n",
      "Test Error for X100 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting: 0.06677796327212016\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "Test Error for X25 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting: 0.09682804674457424\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=100, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=100\n",
      "3rd set of parameters:\n",
      "Test Error for X100 withfeature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting: 0.900946021146355\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=100, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=100\n",
      "Test Error for with feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting: 0.900946021146355\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "4th set of parameters:\n",
      "Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.6388425153032833\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.6766833611574847\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "5th set of parameters:\n",
      "Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.32442960489705064\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.3611574846967167\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######--- LightGBM for X25 and X100 instance datasets---------\n",
    "\n",
    "xtrain25 = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain25 = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest25 = test_df\n",
    "ytest25 = xtest25.copy()\n",
    "xtest25 = xtest25[xtest25.columns[0:64]]\n",
    "ytest25 = ytest25[ytest25.columns[64]]\n",
    "\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "\n",
    "\n",
    "#------  default parameter values training of the dataset--------\n",
    "\n",
    "model = LGBMClassifier()\n",
    "model.fit(xtrain,ytrain)\n",
    "print(\"Parameters and their default values for the LightGBM model:\",model)\n",
    "print('')\n",
    "print('')\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('1st set of parameters:')\n",
    "print(\"Test Error for X100 with default parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with default parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4--------\n",
    "\n",
    "model = LGBMClassifier(learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('2nd set of parameters:')\n",
    "print(\"Test Error for X100 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100--------\n",
    "\n",
    "model = LGBMClassifier(feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('3rd set of parameters:')\n",
    "print(\"Test Error for X100 withfeature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for with feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8--------\n",
    "\n",
    "model = LGBMClassifier(learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('4th set of parameters:')\n",
    "print(\"Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------num_leaves=2,max_depth=1,max_bin=1,min_data_in_leaf=1,learning_rate=0.01--------\n",
    "\n",
    "model = LGBMClassifier(num_leaves=2,max_depth=1,max_bin=2,min_data_in_leaf=1,learning_rate=0.01)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('5th set of parameters:')\n",
    "print(\"Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X25 training dataset) : 0.5854201446855871\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 5 hidden units  (X25 training dataset) : 0.42070116861435725\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X25 training dataset): 0.1435726210350584\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 2 hidden units  (X25 training dataset): 0.666110183639399\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 5 hidden units(X25 training dataset): 0.49972175848636613\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 10 hidden units(X25 training dataset): 0.5114079020589872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xtrain = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X25 training dataset) :\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 5 hidden units  (X25 training dataset) :\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X25 training dataset):\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,2), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 2 hidden units  (X25 training dataset):\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,2), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 5 hidden units(X25 training dataset):\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,2), max_iter=4000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 10 hidden units(X25 training dataset):\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for each of the  16 features: \n",
      "Feature 48 has test error: 0.0\n",
      "Feature 49 has test error: 0.7506902900678881\n",
      "Feature 50 has test error: 2.8864323502738083\n",
      "Feature 51 has test error: 3.2738711098940496\n",
      "Feature 52 has test error: 4.748364068388907\n",
      "Feature 53 has test error: 5.269006636855425\n",
      "Feature 54 has test error: 4.968964452962672\n",
      "Feature 55 has test error: 2.0328789154996105\n",
      "Feature 56 has test error: 0.0\n",
      "Feature 57 has test error: 0.0\n",
      "Feature 58 has test error: 1.151506339899494\n",
      "Feature 59 has test error: 2.6994986233890836\n",
      "Feature 60 has test error: 2.7381083924966836\n",
      "Feature 61 has test error: 3.9056553414771304\n",
      "Feature 62 has test error: 6.197486569489725\n",
      "Feature 63 has test error: 0.8700034927217627\n"
     ]
    }
   ],
   "source": [
    "X100_class69 = train_df[train_df[64] == 6].sample(100)\n",
    "X100_class69 = X100_class69.append(train_df[train_df[64] == 9].sample(100))\n",
    "xtrain = X100_class69[X100_class69.columns[0:47]]\n",
    "test69 = test_df[test_df[64] == 6]\n",
    "test69.append(test_df[test_df[64] == 9])\n",
    "xtest = test69[test69.columns[0:47]]\n",
    "regressormodel = DecisionTreeRegressor(random_state = 0)\n",
    "print(\"Error for each of the  16 features: \")\n",
    "for i in range(48,64):\n",
    "    ytrain = X100_class69[i]\n",
    "    ytest = test69[i]\n",
    "    regressormodel.fit(xtrain, ytrain)\n",
    "    pred = regressormodel.predict(xtest)\n",
    "    print(f\"Feature {i} has test error: {math.sqrt(mean_squared_error(ytest,pred))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
