{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1ca36ff-eeac-48ad-8025-baab412399dd",
   "metadata": {},
   "source": [
    "# Group number : 11\n",
    "# Group members:\n",
    "Abinaya Swaminathan - yf7584\n",
    "Dhruvil Shah - pw9773\n",
    "Megha Dusane - eb3597"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1958eb60-0cb1-46ba-bba9-7d11ca7cc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn import datasets\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62aa87e-4050-44c6-b1be-9c57f75536f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"optdigits.tra\",header=None)\n",
    "test_df = pd.read_csv(\"optdigits.tes\",header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e5e56-ca48-4878-a436-20325994f719",
   "metadata": {},
   "source": [
    "# Question 1 : X25: Randomly chosen N=25 instances from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba2723d-8017-470d-8b59-d79e19b61450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(250, 65)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X25_EachClass = train_df[train_df[64] == 0].sample(25)\n",
    "for i in range(1,10):\n",
    "        X25_EachClass = X25_EachClass.append(train_df[train_df[64] == i].sample(25))\n",
    "X25_EachClass.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf823945-c2c2-402c-819d-6037be6b4715",
   "metadata": {},
   "source": [
    "# Question 1 : X100: Randomly chosen N=100 instances from each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8a289e-ebf7-4b2a-a37f-ce9877c78c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 65)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X100_EachClass = train_df[train_df[64] == 0].sample(100)\n",
    "for i in range(1,10):\n",
    "        X100_EachClass = X100_EachClass.append(train_df[train_df[64] == i].sample(100))\n",
    "X100_EachClass.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1f0b9c-f428-4d68-88c1-a71ccf748d70",
   "metadata": {},
   "source": [
    "# Question 2 : Decision Trees, classification: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a52c3a0-56f0-4269-9e51-35062f504e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for X25 instances with max_depth: 2 is: 0.659432387312187\n",
      "Error for X25 instances with max_depth: 3 is: 0.4741235392320534\n",
      "Error for X25 instances with max_depth: 5 is: 0.3082915971062883\n",
      "Error for X25 instances with max_depth: 10 is: 0.2966054535336672\n",
      "\n",
      "Error for X100 instances with max_depth: 2 is: 0.6488592097941013\n",
      "Error for X100 instances with max_depth: 3 is: 0.4084585420144685\n",
      "Error for X100 instances with max_depth: 5 is: 0.26043405676126874\n",
      "Error for X100 instances with max_depth: 10 is: 0.19309961046188096\n",
      "\n",
      "Error for the entire dataset with max_depth: 2 is: 0.664440734557596\n",
      "Error for the entire dataset with max_depth: 3 is: 0.4652198107957707\n",
      "Error for the entire dataset with max_depth: 5 is: 0.23205342237061766\n",
      "Error for the entire dataset with max_depth: 10 is: 0.1363383416805788\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_depth = [2,3,5,10]\n",
    "\n",
    "##------------- X25 Instances for each class------------------\n",
    "\n",
    "xtrain = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for X25 instances with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "    \n",
    "print('')\n",
    "#--------------X100 Instances for each class-----------------------\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for X100 instances with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "print('')    \n",
    "\n",
    "\n",
    "#--------------Entire training dataset-----------------------\n",
    "\n",
    "xtrain = train_df[train_df.columns[0:64]]\n",
    "ytrain = train_df[train_df.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "for i in max_depth:\n",
    "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=i)\n",
    "    clf = clf.fit(xtrain,ytrain)\n",
    "    y_pred = clf.predict(xtest)\n",
    "    print(\"Error for the entire dataset with max_depth:\",i,\"is:\",1-metrics.accuracy_score(ytest, y_pred))\n",
    "    text_representation = tree.export_text(clf)\n",
    "    #print(text_representation)\n",
    "print('')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c495735e-cc6c-43d1-b9b7-bed3893c7e40",
   "metadata": {},
   "source": [
    "The best depth value with the least error rate is X100 with max_depth=10. As the number of instances change from 25 to 100, the error rate gets minimal and accuracy improves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea157e2e-8343-41e6-b0b3-901194a84c36",
   "metadata": {},
   "source": [
    "# Question 2: XGBOOST and LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1abffbb-fb64-4bbf-84d7-01b17f28d546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:10:14] WARNING: /private/var/folders/zm/mvzqsntd2j59mmyx83w0x6cr0000gn/T/pip-install-bm_uuee5/xgboost_306b62d570b044bbb86406d0a2d13517/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Parameters and their default values for the XGboost model: XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1,\n",
      "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
      "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
      "              tree_method='exact', use_label_encoder=False,\n",
      "              validate_parameters=1, verbosity=None)\n",
      "\n",
      "\n",
      "1st set of parameters:\n",
      "Test Error for X100 with default parameter setting: 0.08069003895381188\n",
      "[13:10:14] WARNING: /private/var/folders/zm/mvzqsntd2j59mmyx83w0x6cr0000gn/T/pip-install-bm_uuee5/xgboost_306b62d570b044bbb86406d0a2d13517/build/temp.macosx-11.0-arm64-3.9/xgboost/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Test Error for X25 with default parameter setting: 0.13132999443516968\n",
      "\n",
      "2nd set of parameters:\n",
      "Test Error for X100 with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0: 0.900946021146355\n",
      "Test Error for X25 with with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0: 0.900946021146355\n",
      "\n",
      "3rd set of parameters:\n",
      "Test Error for X100 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting: 0.08124652198107962\n",
      "Test Error for X25 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting: 0.13132999443516968\n",
      "\n",
      "4th set of parameters:\n",
      "Test Error for X100 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.08180300500834725\n",
      "Test Error for X25 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.14134668892598778\n",
      "\n",
      "5th set of parameters:\n",
      "Test Error for X100 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.08458542014468562\n",
      "Test Error for X25 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting: 0.12186978297161932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########--------XGBOOST for X100 and X25 with different parameter settings------\n",
    "\n",
    "\n",
    "xtrain25 = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain25 = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest25 = test_df\n",
    "ytest25 = xtest25.copy()\n",
    "xtest25 = xtest25[xtest25.columns[0:64]]\n",
    "ytest25 = ytest25[ytest25.columns[64]]\n",
    "\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "\n",
    "\n",
    "#------  default parameter values training of the dataset--------\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(xtrain,ytrain)\n",
    "print(\"Parameters and their default values for the XGboost model:\",model)\n",
    "print('')\n",
    "print('')\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('1st set of parameters:')\n",
    "print(\"Test Error for X100 with default parameter setting:\",error)\n",
    "\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False)\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with default parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#---------booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=5,min_child_weight=2,subsample=0,verbosity=0)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('2nd set of parameters:')\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with with booster=dart,max_depth=5,min_child_weight=2,subsample=0,verbosity=0:\", error25)\n",
    "print('')\n",
    "\n",
    "#---------booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('3rd set of parameters:')\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting:\", error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with booster=dart,max_depth=2,min_child_weight=4,max_delta_step=3,eta=0.5 parameter setting:\", error25)\n",
    "print('')\n",
    "#---------tree_method=exact,max_depth=7,min_child_weight=6,scale_pos_weight=0,refresh_leaf=0\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,tree_method='exact',max_depth=7,min_child_weight=6,scale_pos_weight=0,refresh_leaf=0)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('4th set of parameters:')\n",
    "print(\"Test Error for X100 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with colsample_bytree=0,max_depth=7,min_child_weight=6,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error25)\n",
    "print('')\n",
    "#---------booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "model = XGBClassifier(use_label_encoder=False,booster='dart',max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('5th set of parameters:')\n",
    "print(\"Test Error for X100 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error)\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with booster=dart,max_depth=1,min_child_weight=1,colsample_bynode=1,colsample_bylevel=1 parameter setting:\", error25)\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb2b79-94cc-4754-906f-6cf8ba8f7597",
   "metadata": {},
   "source": [
    "As the number of training instances change and with varied best parameters, the error rate for these parameters reduces for increased instances. \n",
    "The best parameter setting from my observation with minimal error rate is for the default parameter setting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5010c488-a023-43de-aecd-ad6d8418a70b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st set of parameters:\n",
      "Test Error for X100 with default parameter setting: 0.06176961602671116\n",
      "Test Error for X25 with default parameter setting: 0.1062882582081246\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "2nd set of parameters:\n",
      "Test Error for X100 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting: 0.06677796327212016\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=4, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=4\n",
      "Test Error for X25 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting: 0.12298274902615469\n",
      "\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=100, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=100\n",
      "3rd set of parameters:\n",
      "Test Error for X100 withfeature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting: 0.900946021146355\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=100, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=100\n",
      "Test Error for with feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting: 0.900946021146355\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "4th set of parameters:\n",
      "Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.6043405676126878\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=8, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=8\n",
      "Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.7289927657206455\n",
      "\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "5th set of parameters:\n",
      "Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.3494713411240957\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=1, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=1\n",
      "Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting: 0.34335002782415136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "######--- LightGBM for X25 and X100 instance datasets---------\n",
    "\n",
    "xtrain25 = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain25 = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest25 = test_df\n",
    "ytest25 = xtest25.copy()\n",
    "xtest25 = xtest25[xtest25.columns[0:64]]\n",
    "ytest25 = ytest25[ytest25.columns[64]]\n",
    "\n",
    "\n",
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "\n",
    "\n",
    "#------  default parameter values training of the dataset--------\n",
    "\n",
    "model = LGBMClassifier()\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('1st set of parameters:')\n",
    "print(\"Test Error for X100 with default parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with default parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4--------\n",
    "\n",
    "model = LGBMClassifier(learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('2nd set of parameters:')\n",
    "print(\"Test Error for X100 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 learning_rate=0.2,num_leaves=3,max_depth=7,max_bin=4,min_data_in_leaf=4 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100--------\n",
    "\n",
    "model = LGBMClassifier(feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('3rd set of parameters:')\n",
    "print(\"Test Error for X100 withfeature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for with feature_fraction=0.1,num_leaves=5,max_depth=10,max_bin=6,min_sum_hessian_in_leaf=100 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8--------\n",
    "\n",
    "model = LGBMClassifier(learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('4th set of parameters:')\n",
    "print(\"Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "#------num_leaves=2,max_depth=1,max_bin=1,min_data_in_leaf=1,learning_rate=0.01--------\n",
    "\n",
    "model = LGBMClassifier(num_leaves=2,max_depth=1,max_bin=2,min_data_in_leaf=1,learning_rate=0.01)\n",
    "model.fit(xtrain,ytrain)\n",
    "#--- Predictions for the testing model\n",
    "y_pred = model.predict(xtest)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "error = 1-metrics.accuracy_score(ytest, predictions)\n",
    "print('5th set of parameters:')\n",
    "print(\"Test Error for X100 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\",error)\n",
    "\n",
    "\n",
    "model.fit(xtrain25,ytrain25)\n",
    "#--- Predictions for the testing model\n",
    "y_pred25 = model.predict(xtest25)\n",
    "predictions25 = [round(value) for value in y_pred25]\n",
    "error25 = 1-metrics.accuracy_score(ytest25, predictions25)\n",
    "print(\"Test Error for X25 with learning_rate=0.8,num_leaves=8,max_depth=8,max_bin=8,min_data_in_leaf=8 parameter setting:\", error25)\n",
    "print('')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bfaf4-e9ce-49e0-b18b-1fb5ca5cb396",
   "metadata": {},
   "source": [
    "As the number of training instances change and with varied best parameters, the error rate for these parameters reduces for increased instances. \n",
    " \n",
    "The best parameter setting from my observation with minimal error rate is for the default parameter setting\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c1b97-1bb1-49d7-a0f2-2c3e12f5b050",
   "metadata": {},
   "source": [
    "# Question 3 Multilayer Perceptrons: classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3552e229-863b-4c7f-b75e-cd5d7384015d",
   "metadata": {},
   "source": [
    "X25 Intances training dataset and predicting the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51d80f52-0825-4570-bcc4-590a77d622e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (3000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X25 training dataset) : 0.5592654424040067\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 5 hidden units  (X25 training dataset) : 0.24930439621591538\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X25 training dataset): 0.19254312743461321\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 2 hidden units  (X25 training dataset): 0.7540345019476906\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 5 hidden units(X25 training dataset): 0.45909849749582643\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 10 hidden units(X25 training dataset): 0.5754034501947691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xtrain = X25_EachClass[X25_EachClass.columns[0:64]]\n",
    "ytrain = X25_EachClass[X25_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X25 training dataset) :\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,), max_iter=3002,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 5 hidden units  (X25 training dataset) :\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1403,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X25 training dataset):\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,2), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 2 hidden units  (X25 training dataset):\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,2), max_iter=3000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 5 hidden units(X25 training dataset):\",1- metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,2), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 10 hidden units(X25 training dataset):\", 1-metrics.accuracy_score(ytest,y_pred))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2201db09-3076-4596-b5c5-7dca231ad141",
   "metadata": {},
   "source": [
    "The MLP Classifier gives the best test error for 1 hidden layer and 10 hidden units for the X25 instance training dataset and the error is approximately around 0.1720. From the above screenshot we can say that the error rate increases as the hidden layer increases and the error rate decreases as the hidden unit increases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da5d9c-0025-4681-946c-f8fb44bfe7f5",
   "metadata": {},
   "source": [
    "X100 Intances training dataset and predicting the test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dea57d6c-125f-4e1a-b527-ef56967c56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X100 training dataset) : 0.5275459098497496\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 5 hidden units (X100 training dataset) : 0.8664440734557596\n",
      "\n",
      "Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X100 training dataset): 0.8831385642737897\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 2 hidden units (X100 training dataset) : 0.44407345575959933\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 5 hidden units (X100 training dataset): 0.7762938230383973\n",
      "\n",
      "Test Error of MLPClassifier with 2 hidden layer and 10 hidden units (X100 training dataset): 0.5854201446855871\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xtrain = X100_EachClass[X100_EachClass.columns[0:64]]\n",
    "ytrain = X100_EachClass[X100_EachClass.columns[64]]\n",
    "xtest = test_df\n",
    "ytest = xtest.copy()\n",
    "xtest = xtest[xtest.columns[0:64]]\n",
    "ytest = ytest[ytest.columns[64]]\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 2 hidden units (X100 training dataset) :\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 5 hidden units (X100 training dataset) :\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1403,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 1 hidden layer and 10 hidden units (X100 training dataset):\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(2,2), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 2 hidden units (X100 training dataset) :\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(5,2), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 5 hidden units (X100 training dataset):\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')\n",
    "\n",
    "classifier = MLPClassifier(hidden_layer_sizes=(10,2), max_iter=2000,activation = 'relu',solver='adam',random_state=1)\n",
    "#Fitting the training data to the network\n",
    "classifier.fit(xtrain, ytrain)\n",
    "#Predicting y for X_Test\n",
    "y_pred = classifier.predict(xtest)\n",
    "#Printing the accuracy\n",
    "print(\"Test Error of MLPClassifier with 2 hidden layer and 10 hidden units (X100 training dataset):\", metrics.accuracy_score(ytest,y_pred))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9183e285-3454-4a58-ae83-86b123837dd5",
   "metadata": {},
   "source": [
    "The MLP Classifier gives the best test error for 1 hidden layer and 2 hidden units for the X100 instance training dataset and the error is approximately around 0.097.\n",
    "From the screenshot we can notice that the error rate increases for the increased values of hidden layers and hidden units.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c9931d-fd5b-4abf-b50f-93fad0dcad63",
   "metadata": {},
   "source": [
    "# Question 4: Regression for digit completion: regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929a1bcd-b7dc-4c11-8984-59a0044942f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for each of the  16 features: \n",
      "Feature 48 has test error: 0.0\n",
      "Feature 49 has test error: 0.8950446761897926\n",
      "Feature 50 has test error: 4.004831336417199\n",
      "Feature 51 has test error: 3.7246379927176254\n",
      "Feature 52 has test error: 4.871838116135865\n",
      "Feature 53 has test error: 4.692182283930938\n",
      "Feature 54 has test error: 4.213894407474879\n",
      "Feature 55 has test error: 1.8069846070638638\n",
      "Feature 56 has test error: 0.0\n",
      "Feature 57 has test error: 0.0\n",
      "Feature 58 has test error: 1.1869455669935016\n",
      "Feature 59 has test error: 2.744155013903459\n",
      "Feature 60 has test error: 2.274042813632154\n",
      "Feature 61 has test error: 3.7534514503394405\n",
      "Feature 62 has test error: 4.942207438447003\n",
      "Feature 63 has test error: 0.8668224820974668\n"
     ]
    }
   ],
   "source": [
    "X100_class69 = train_df[train_df[64] == 6].sample(100)\n",
    "X100_class69 = X100_class69.append(train_df[train_df[64] == 9].sample(100))\n",
    "xtrain = X100_class69[X100_class69.columns[0:47]]\n",
    "test69 = test_df[test_df[64] == 6]\n",
    "test69.append(test_df[test_df[64] == 9])\n",
    "xtest = test69[test69.columns[0:47]]\n",
    "regressormodel = DecisionTreeRegressor(random_state = 0)\n",
    "print(\"Error for each of the  16 features: \")\n",
    "for i in range(48,64):\n",
    "    ytrain = X100_class69[i]\n",
    "    ytest = test69[i]\n",
    "    regressormodel.fit(xtrain, ytrain)\n",
    "    pred = regressormodel.predict(xtest)\n",
    "    print(f\"Feature {i} has test error: {math.sqrt(mean_squared_error(ytest,pred))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb4f76-394f-48c8-aa24-df9ed1455dd8",
   "metadata": {},
   "source": [
    "Pixels 48, 56 and 57 are easier to predict with 0 error rate and feature 63 and 49 with a minimal error rate\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cd0d92-e4b0-46f1-b043-87ade333f4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
